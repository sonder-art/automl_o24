{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a PyTorch: Tensores, GPU, y Benchmarks\n",
    "\n",
    "Bienvenidos a este cuaderno interactivo diseñado para introducir a los estudiantes en PyTorch, el manejo de tensores, el uso de GPU, y la realización de benchmarks. A lo largo de este cuaderno, exploraremos desde los fundamentos de PyTorch hasta la definición de una red neuronal básica, enfocándonos en la comprensión de cómo funcionan los tensores en CPU y GPU, así como en la evaluación de su rendimiento y uso de memoria.\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Instalación y Configuración](#instalación-y-configuración)\n",
    "2. [Conceptos Básicos de PyTorch](#conceptos-básicos-de-pytorch)\n",
    "3. [Tensores en PyTorch](#tensores-en-pytorch)\n",
    "    - [Creación de Tensores](#creación-de-tensores)\n",
    "    - [Atributos de los Tensores](#atributos-de-los-tensores)\n",
    "    - [Operaciones Básicas con Tensores](#operaciones-básicas-con-tensores)\n",
    "    - [Más sobre Tensores y Dimensiones](#más-sobre-tensores-y-dimensiones)\n",
    "        - [Indexación y Slicing](#indexación-y-slicing)\n",
    "        - [Cambiar la Forma y Dimensiones](#cambiar-la-forma-y-dimensiones)\n",
    "        - [Operaciones Avanzadas](#operaciones-avanzadas)\n",
    "        - [Broadcasting](#broadcasting)\n",
    "4. [GPU vs CPU](#gpu-vs-cpu)\n",
    "    - [Uso de GPU en PyTorch](#uso-de-gpu-en-pytorch)\n",
    "    - [Comunicación entre CPU y GPU](#comunicación-entre-cpu-y-gpu)\n",
    "5. [Benchmarks de Rendimiento](#benchmarks-de-rendimiento)\n",
    "    - [Benchmark de Tiempo de Ejecución](#benchmark-de-tiempo-de-ejecución)\n",
    "    - [Benchmark de Uso de Memoria](#benchmark-de-uso-de-memoria)\n",
    "    - [Precisión y Rendimiento](#precisión-y-rendimiento)\n",
    "    - [Benchmark Extendido: Diferentes Dimensiones y Precisión](#benchmark-extendido-diferentes-dimensiones-y-precisión)\n",
    "6. [Definición de una Red Neuronal Básica](#definición-de-una-red-neuronal-básica)\n",
    "    - [Arquitectura de la Red](#arquitectura-de-la-red)\n",
    "    - [Forward Pass](#forward-pass)\n",
    "7. [Conclusiones](#conclusiones)\n",
    "\n",
    "---\n",
    "\n",
    "## Instalación y Configuración\n",
    "\n",
    "Antes de comenzar, asegurémonos de tener instalado PyTorch y los paquetes necesarios. Puedes instalar PyTorch siguiendo las instrucciones oficiales en [pytorch.org](https://pytorch.org/get-started/locally/). A continuación, verificaremos la instalación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar la instalación de PyTorch\n",
    "import torch\n",
    "\n",
    "print(f\"Versión de PyTorch: {torch.__version__}\")\n",
    "print(f\"¿CUDA está disponible?: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptos Básicos de PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch es una biblioteca de aprendizaje automático de código abierto desarrollada por Facebook. Es ampliamente utilizada para aplicaciones de procesamiento de lenguaje natural y visión por computadora. PyTorch proporciona dos características de alto nivel:\n",
    "\n",
    "    Computación de tensores con aceleración en GPU.\n",
    "    Redes neuronales profundas construidas de manera modular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tensores en PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tensores son la estructura de datos fundamental en PyTorch, similares a los arrays de NumPy pero con la capacidad de ser procesados en GPUs.\n",
    "Creación de Tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear un tensor a partir de una lista\n",
    "tensor_lista = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"Tensor desde lista:\", tensor_lista)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un tensor de ceros\n",
    "tensor_ceros = torch.zeros((3, 3))\n",
    "print(\"Tensor de ceros:\\n\", tensor_ceros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un tensor de unos\n",
    "tensor_unos = torch.ones((2, 4))\n",
    "print(\"Tensor de unos:\\n\", tensor_unos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un tensor aleatorio\n",
    "tensor_aleatorio = torch.rand((2, 3))\n",
    "print(\"Tensor aleatorio:\\n\", tensor_aleatorio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atributos de los Tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un tensor de ejemplo\n",
    "tensor = torch.randn(3, 4)\n",
    "\n",
    "print(\"Tensor:\\n\", tensor)\n",
    "print(\"Forma del tensor:\", tensor.shape)\n",
    "print(\"Tipo de datos:\", tensor.dtype)\n",
    "print(\"Dispositivo:\", tensor.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones Básicas con Tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suma de tensores\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "suma = a + b\n",
    "print(\"Suma:\", suma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producto punto\n",
    "producto = torch.dot(a, b)\n",
    "print(\"Producto punto:\", producto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Producto matricial\n",
    "mat_a = torch.randn(2, 3)\n",
    "mat_b = torch.randn(3, 2)\n",
    "producto_mat = torch.matmul(mat_a, mat_b)\n",
    "print(\"Producto matricial:\\n\", producto_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redimensionar un tensor\n",
    "tensor_original = torch.randn(4, 4)\n",
    "tensor_redimensionado = tensor_original.view(16)\n",
    "print(\"Tensor redimensionado:\", tensor_redimensionado.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Más sobre Tensores y Dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, profundizaremos en el manejo de dimensiones de los tensores, incluyendo indexación, slicing, cambio de forma, operaciones avanzadas y broadcasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexación y Slicing**\n",
    "\n",
    "La indexación y slicing permiten acceder y modificar partes específicas de un tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un tensor de 3 dimensiones\n",
    "tensor_3d = torch.arange(27).reshape(3, 3, 3)\n",
    "print(\"Tensor 3D:\\n\", tensor_3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a un elemento específico\n",
    "elemento = tensor_3d[1, 1, 1]\n",
    "print(\"Elemento en [1,1,1]:\", elemento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Slicing en la primera dimensión\n",
    "slice_0 = tensor_3d[0, :, :]\n",
    "print(\"Slice de la primera dimensión:\\n\", slice_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing con pasos\n",
    "tensor_pasos = torch.arange(10)\n",
    "print(\"Tensor con pasos:\", tensor_pasos)\n",
    "slice_pasos = tensor_pasos[::2]\n",
    "print(\"Slice con pasos de 2:\", slice_pasos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cambiar la Forma y Dimensiones**\n",
    "\n",
    "Modificar la forma de un tensor es una operación común para preparar datos para modelos de aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un tensor de 2x3\n",
    "tensor_2x3 = torch.randn(2, 3)\n",
    "print(\"Tensor 2x3:\\n\", tensor_2x3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar a 3x2\n",
    "tensor_3x2 = tensor_2x3.view(3, 2)\n",
    "print(\"Tensor 3x2:\\n\", tensor_3x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir una dimensión\n",
    "tensor_1d = torch.tensor([1, 2, 3, 4])\n",
    "tensor_2d = tensor_1d.unsqueeze(0)\n",
    "print(\"Tensor 2D después de unsqueeze:\\n\", tensor_2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar una dimensión\n",
    "tensor_squeeze = tensor_2d.squeeze()\n",
    "print(\"Tensor después de squeeze:\", tensor_squeeze.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Operaciones Avanzadas**\n",
    "\n",
    "Exploraremos algunas operaciones más avanzadas que se pueden realizar con tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposición de un tensor\n",
    "tensor_matriz = torch.randn(2, 3)\n",
    "print(\"Tensor original:\\n\", tensor_matriz)\n",
    "tensor_transpuesto = tensor_matriz.t()\n",
    "print(\"Tensor transpuesto:\\n\", tensor_transpuesto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenación de tensores\n",
    "tensor_a = torch.randn(2, 3)\n",
    "tensor_b = torch.randn(2, 3)\n",
    "concatenado = torch.cat((tensor_a, tensor_b), dim=0)\n",
    "print(\"Tensores concatenados en dim=0:\\n\", concatenado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apilamiento de tensores\n",
    "apilado = torch.stack((tensor_a, tensor_b), dim=0)\n",
    "print(\"Tensores apilados en dim=0:\\n\", apilado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de tensores\n",
    "tensor_div = torch.chunk(concatenado, 2, dim=0)\n",
    "print(\"Tensores divididos:\")\n",
    "for chunk in tensor_div:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU vs CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si CUDA está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mover un tensor a GPU\n",
    "if torch.cuda.is_available():\n",
    "    tensor_gpu = torch.randn(1000, 1000).to(device)\n",
    "    print(\"Tensor en GPU:\", tensor_gpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comunicación entre CPU y GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante entender cómo transferir datos entre la CPU y la GPU para maximizar el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un tensor en CPU\n",
    "tensor_cpu = torch.randn(1000, 1000)\n",
    "print(\"Tensor en CPU:\", tensor_cpu.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos que manejar la comunicacion entre GPU y CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mover tensor a GPU\n",
    "if torch.cuda.is_available():\n",
    "    tensor_gpu = tensor_cpu.to(device)\n",
    "    print(\"Tensor en GPU:\", tensor_gpu.device)\n",
    "\n",
    "    # Realizar una operación en GPU\n",
    "    resultado_gpu = tensor_gpu * 2\n",
    "    print(\"Resultado en GPU:\", resultado_gpu.device)\n",
    "\n",
    "    # Mover el resultado de vuelta a CPU\n",
    "    resultado_cpu = resultado_gpu.to(\"cpu\")\n",
    "    print(\"Resultado de vuelta en CPU:\", resultado_cpu.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks de Rendimiento\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos algunos benchmarks para comparar el rendimiento entre CPU y GPU, así como el uso de memoria y precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark de Tiempo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_tiempo(tamaño, dispositivo):\n",
    "    tensor_a = torch.randn(tamaño, tamaño, device=dispositivo)\n",
    "    tensor_b = torch.randn(tamaño, tamaño, device=dispositivo)\n",
    "    \n",
    "    # Sincronizar para obtener medidas precisas en GPU\n",
    "    if dispositivo == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    inicio = time.time()\n",
    "    resultado = torch.matmul(tensor_a, tensor_b)\n",
    "    \n",
    "    # Sincronizar nuevamente\n",
    "    if dispositivo == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    fin = time.time()\n",
    "    \n",
    "    print(f\"Multiplicación de matrices de tamaño {tamaño}x{tamaño} en {dispositivo}: {fin - inicio:.4f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamaños para el benchmark\n",
    "tamaños = [500, 1000, 1500]\n",
    "\n",
    "for tamaño in tamaños:\n",
    "    benchmark_tiempo(tamaño, \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        benchmark_tiempo(tamaño, \"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark de Uso de Memoria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_memoria(tamaño, dispositivo):\n",
    "    if dispositivo == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    tensor = torch.randn(tamaño, tamaño, device=dispositivo)\n",
    "    \n",
    "    if dispositivo == \"cuda\":\n",
    "        memoria = torch.cuda.max_memory_allocated(device)\n",
    "        print(f\"Uso de memoria en {dispositivo} para tamaño {tamaño}x{tamaño}: {memoria / 1e6:.2f} MB\")\n",
    "    else:\n",
    "        # Para CPU, usamos el atributo 'element_size' y 'numel'\n",
    "        memoria = tensor.element_size() * tensor.numel()\n",
    "        print(f\"Uso de memoria en {dispositivo} para tamaño {tamaño}x{tamaño}: {memoria / 1e6:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamaños para el benchmark de memoria\n",
    "tamaños_mem = [1000, 2000, 3000]\n",
    "\n",
    "for tamaño in tamaños_mem:\n",
    "    benchmark_memoria(tamaño, \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        benchmark_memoria(tamaño, \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precisión y Rendimiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compararemos el rendimiento y el uso de memoria entre diferentes precisiones de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_precision(tamaño, dispositivo):\n",
    "    precisiones = {\n",
    "        \"float32\": torch.float32,\n",
    "        \"float16\": torch.float16,\n",
    "        \"int8\": torch.int8  # Nota: operaciones en int8 son limitadas\n",
    "    }\n",
    "    \n",
    "    for nombre, dtype in precisiones.items():\n",
    "        try:\n",
    "            tensor_a = torch.randn(tamaño, tamaño, device=dispositivo, dtype=dtype)\n",
    "            tensor_b = torch.randn(tamaño, tamaño, device=dispositivo, dtype=dtype)\n",
    "            \n",
    "            # Sincronizar para GPU\n",
    "            if dispositivo == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            inicio = time.time()\n",
    "            resultado = torch.matmul(tensor_a, tensor_b)\n",
    "            \n",
    "            # Sincronizar nuevamente\n",
    "            if dispositivo == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            fin = time.time()\n",
    "            \n",
    "            if dispositivo == \"cuda\":\n",
    "                memoria = torch.cuda.max_memory_allocated(device)\n",
    "                print(f\"{nombre} en {dispositivo}: Tiempo = {fin - inicio:.4f} s, Memoria = {memoria / 1e6:.2f} MB\")\n",
    "            else:\n",
    "                memoria = resultado.element_size() * resultado.numel()\n",
    "                print(f\"{nombre} en {dispositivo}: Tiempo = {fin - inicio:.4f} s, Memoria = {memoria / 1e6:.2f} MB\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error con precisión {nombre} en {dispositivo}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar benchmark de precisión\n",
    "tamaño_prec = 1000\n",
    "benchmark_precision(tamaño_prec, \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    benchmark_precision(tamaño_prec, \"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: PyTorch tiene soporte limitado para operaciones en int8. Por lo tanto, algunas operaciones pueden no estar disponibles o pueden producir errores, sobretodo para CPU. El GPU tiene menos problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark Extendido: Diferentes Dimensiones y Precisión**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este benchmark, evaluaremos el rendimiento y el uso de memoria para diferentes dimensiones de tensores y precisiones de datos (8, 16, 32, 64 bits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_extendido(dimensiones, precisiones, dispositivo):\n",
    "    print(f\"\\nBenchmark en {dispositivo}\")\n",
    "    for dim in dimensiones:\n",
    "        for nombre, dtype in precisiones.items():\n",
    "            try:\n",
    "                # Crear tensores con la precisión y dimensión especificadas\n",
    "                tensor_a = torch.randn(dim, dim, device=dispositivo, dtype=dtype)\n",
    "                tensor_b = torch.randn(dim, dim, device=dispositivo, dtype=dtype)\n",
    "                \n",
    "                # Sincronizar para GPU\n",
    "                if dispositivo == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                inicio = time.time()\n",
    "                resultado = torch.matmul(tensor_a, tensor_b)\n",
    "                \n",
    "                # Sincronizar nuevamente\n",
    "                if dispositivo == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                fin = time.time()\n",
    "                \n",
    "                # Calcular uso de memoria\n",
    "                if dispositivo == \"cuda\":\n",
    "                    memoria = torch.cuda.max_memory_allocated(device)\n",
    "                    memoria_MB = memoria / 1e6\n",
    "                else:\n",
    "                    memoria = resultado.element_size() * resultado.numel()\n",
    "                    memoria_MB = memoria / 1e6\n",
    "                \n",
    "                print(f\"Dimensión: {dim}x{dim}, Precisión: {nombre}, Tiempo: {fin - inicio:.4f} s, Memoria: {memoria_MB:.2f} MB\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error con dimensión {dim}x{dim} y precisión {nombre} en {dispositivo}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir dimensiones y precisiones\n",
    "dimensiones = [500, 1000, 1500]\n",
    "precisiones = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"float16\": torch.float16,\n",
    "    \"int8\": torch.int8,\n",
    "    \"float64\": torch.float64\n",
    "}\n",
    "\n",
    "# Ejecutar benchmark extendido en CPU\n",
    "benchmark_extendido(dimensiones, precisiones, \"cpu\")\n",
    "\n",
    "# Ejecutar benchmark extendido en GPU si está disponible\n",
    "if torch.cuda.is_available():\n",
    "    benchmark_extendido(dimensiones, precisiones, \"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notas:\n",
    "\n",
    "    Las operaciones en int8 pueden no estar soportadas para todas las dimensiones y dispositivos, lo que podría generar errores.\n",
    "    float64 (double precision) generalmente utiliza más memoria y puede ser más lento que float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación:  \n",
    "    `torch`: Es la librería principal de PyTorch que nos permite manejar tensores y realizar operaciones matemáticas.  \n",
    "    `torch.nn`: Submódulo de PyTorch que contiene herramientas para construir redes neuronales, como capas y funciones de activación.  \n",
    "    `torch.nn.functional`: Proporciona funciones de activación y otras operaciones que no requieren mantener estado, a diferencia de las capas definidas en torch.nn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de la Clase de la Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En PyTorch, las redes neuronales se definen como clases que heredan de nn.Module. Esto nos permite aprovechar las funcionalidades que PyTorch ofrece para gestionar parámetros y realizar operaciones de manera eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronalBasica(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RedNeuronalBasica, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Definición de las capas de la red\n",
    "        self.capa1 = nn.Linear(self.input_size, self.hidden_size)  # Capa completamente conectada\n",
    "        self.activacion = nn.ReLU()                                 # Función de activación ReLU\n",
    "        self.capa2 = nn.Linear(self.hidden_size, self.output_size) # Capa de salida completamente conectada\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define el forward pass de la red.\n",
    "        \"\"\"\n",
    "        # Paso 1: Aplicar la primera capa lineal\n",
    "        out = self.capa1(x)\n",
    "        print(f\"Después de capa1 (Linear): {out}\")\n",
    "        \n",
    "        # Paso 2: Aplicar la función de activación\n",
    "        out = self.activacion(out)\n",
    "        print(f\"Después de activacion (ReLU): {out}\")\n",
    "        \n",
    "        # Paso 3: Aplicar la segunda capa lineal\n",
    "        out = self.capa2(out)\n",
    "        print(f\"Después de capa2 (Linear): {out}\")\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de la Clase:  \n",
    "        `class RedNeuronalBasica(nn.Module)`: Definimos una clase llamada RedNeuronalBasica que hereda de nn.Module, lo que nos permite utilizar todas las funcionalidades de PyTorch para redes neuronales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método __init__:\n",
    "        \n",
    "        def __init__(self, input_size, hidden_size, output_size): El constructor de la clase recibe tres parámetros que definen el tamaño de las capas de la red:\n",
    "            input_size: Número de características de entrada.\n",
    "            hidden_size: Número de neuronas en la capa oculta.\n",
    "            output_size: Número de neuronas en la capa de salida.\n",
    "        \n",
    "`super(RedNeuronalBasica, self).__init__()`: Llamamos al constructor de la clase base nn.Module para inicializar correctamente el módulo.  \n",
    "\n",
    "`self.capa1 = nn.Linear(self.input_size, self.hidden_size)`: Definimos la primera capa completamente conectada que transforma los datos de entrada al tamaño de la capa oculta.  \n",
    "\n",
    "`self.activacion = nn.ReLU()`: Definimos la función de activación ReLU, que introduce no linealidad en el modelo.  \n",
    "\n",
    "`self.capa2 = nn.Linear(self.hidden_size, self.output_size)`: Definimos la segunda capa completamente conectada que transforma los datos de la capa oculta al tamaño de la capa de salida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método forward:  \n",
    "    `def forward(self, x)`: Define cómo pasan los datos a través de la red.  \n",
    "    `out = self.capa1(x)`: Aplica la primera capa lineal a la entrada x.  \n",
    "    `out = self.activacion(out)`: Aplica la función de activación ReLU al resultado de la primera capa.  \n",
    "    `out = self.capa2(out)`: Aplica la segunda capa lineal al resultado de la activación.  `\n",
    "    `return out`: Devuelve la salida final de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciación de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir tamaños de las capas\n",
    "input_size = 784   # Por ejemplo, para imágenes de 28x28 píxeles aplanadas\n",
    "hidden_size = 128  # Número de neuronas en la capa oculta\n",
    "output_size = 10   # Número de clases para la clasificación\n",
    "\n",
    "# Crear una instancia de la red\n",
    "red = RedNeuronalBasica(input_size, hidden_size, output_size)\n",
    "print(red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los tamaños de entrada, ocultos y de salida según el problema que queremos resolver.  \n",
    "En este ejemplo, input_size es 784, lo que podría corresponder a imágenes de 28x28 píxeles aplanadas en un vector de características.  \n",
    "\n",
    "`red = RedNeuronalBasica(input_size, hidden_size, output_size)`: Creamos una instancia de nuestra red neuronal con los tamaños definidos.  \n",
    "`print(red)`: Imprimimos la estructura de la red para verificar que se ha construido correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass con Datos de Ejemplo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos un forward pass utilizando un batch de datos de ejemplo para ver cómo los datos fluyen a través de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el tamaño del batch\n",
    "batch_size = 64\n",
    "\n",
    "# Crear un batch de datos de entrada aleatorios\n",
    "entrada = torch.randn(batch_size, input_size)\n",
    "print(f\"Entrada: {entrada.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de Dispositivos: CPU y GPU (CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aprovechar el poder de las GPUs, PyTorch permite mover tanto los modelos como los datos entre la CPU y la GPU. A continuación, veremos cómo hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si CUDA está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mover la Red y los Datos al Dispositivo Seleccionado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mover la red al dispositivo seleccionado\n",
    "red.to(device)\n",
    "print(f\"Red en dispositivo: {next(red.parameters()).device}\")\n",
    "\n",
    "# Mover los datos de entrada al dispositivo seleccionado\n",
    "entrada = entrada.to(device)\n",
    "print(f\"Entrada en dispositivo: {entrada.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`red.to(device)`: Mueve todos los parámetros y buffers de la red al dispositivo especificado (cuda o cpu).  \n",
    "`next(red.parameters()).device`: Obtiene el dispositivo del primer parámetro de la red para verificar dónde está ubicada.  \n",
    "`entrada = entrada.to(device)`: Mueve el tensor de entrada al mismo dispositivo que la red.  \n",
    "`print(f\"Entrada en dispositivo: {entrada.device}\")`: Imprime el dispositivo donde está el tensor de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar el Forward Pass en el Dispositivo Seleccionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar el forward pass\n",
    "salida = red(entrada)\n",
    "print(f\"Salida de la red: {salida.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definición de la clase de la red neuronal\n",
    "class RedNeuronalBasica(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RedNeuronalBasica, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Definición de las capas de la red\n",
    "        self.capa1 = nn.Linear(self.input_size, self.hidden_size)  # Capa completamente conectada\n",
    "        self.activacion = nn.ReLU()                                 # Función de activación ReLU\n",
    "        self.capa2 = nn.Linear(self.hidden_size, self.output_size) # Capa de salida completamente conectada\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define el forward pass de la red.\n",
    "        \"\"\"\n",
    "        # Paso 1: Aplicar la primera capa lineal\n",
    "        out = self.capa1(x)\n",
    "        print(f\"Después de capa1 (Linear): {out}\")\n",
    "        \n",
    "        # Paso 2: Aplicar la función de activación\n",
    "        out = self.activacion(out)\n",
    "        print(f\"Después de activacion (ReLU): {out}\")\n",
    "        \n",
    "        # Paso 3: Aplicar la segunda capa lineal\n",
    "        out = self.capa2(out)\n",
    "        print(f\"Después de capa2 (Linear): {out}\")\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir tamaños de las capas\n",
    "input_size = 784   # Por ejemplo, para imágenes de 28x28 píxeles aplanadas\n",
    "hidden_size = 128  # Número de neuronas en la capa oculta\n",
    "output_size = 10   # Número de clases para la clasificación\n",
    "\n",
    "# Crear una instancia de la red\n",
    "red = RedNeuronalBasica(input_size, hidden_size, output_size)\n",
    "print(\"Estructura de la Red Neuronal:\")\n",
    "print(red)\n",
    "\n",
    "# Definir el tamaño del batch\n",
    "batch_size = 64\n",
    "\n",
    "# Crear un batch de datos de entrada aleatorios\n",
    "entrada = torch.randn(batch_size, input_size)\n",
    "print(f\"\\nEntrada: {entrada.shape}\")\n",
    "\n",
    "# Verificar si CUDA está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsando dispositivo: {device}\")\n",
    "\n",
    "# Mover la red al dispositivo seleccionado\n",
    "red.to(device)\n",
    "print(f\"Red en dispositivo: {next(red.parameters()).device}\")\n",
    "\n",
    "# Mover los datos de entrada al dispositivo seleccionado\n",
    "entrada = entrada.to(device)\n",
    "print(f\"Entrada en dispositivo: {entrada.device}\")\n",
    "\n",
    "# Realizar el forward pass\n",
    "print(\"\\nRealizando el forward pass:\")\n",
    "salida = red(entrada)\n",
    "print(f\"Salida de la red: {salida.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estructura de la Red Neuronal:\")\n",
    "print(red)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo de Acceso a los Parámetros de la Red**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a los parámetros de la red\n",
    "for nombre_param, param in red.named_parameters():\n",
    "    print(f\"Nombre del parámetro: {nombre_param}\")\n",
    "    print(f\"Valor:\\n{param}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los pesos de la red\n",
    "torch.save(red.state_dict(), 'red_neuronal.pth')\n",
    "print(\"Modelo guardado en 'red_neuronal.pth'\")\n",
    "\n",
    "# Crear una nueva instancia de la red\n",
    "nueva_red = RedNeuronalBasica(input_size, hidden_size, output_size)\n",
    "\n",
    "# Cargar los pesos en la nueva red\n",
    "nueva_red.load_state_dict(torch.load('red_neuronal.pth'))\n",
    "nueva_red.to(device)\n",
    "print(\"Modelo cargado y movido al dispositivo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.save(red.state_dict(), 'red_neuronal.pth')`: Guarda los parámetros de la red en un archivo llamado red_neuronal.pth.  \n",
    "`nueva_red = RedNeuronalBasica(input_size, hidden_size, output_size)`: Crea una nueva instancia de la red.\n",
    "`nueva_red.load_state_dict(torch.load('red_neuronal.pth'))`: Carga los parámetros guardados en la nueva instancia de la red.  \n",
    "`nueva_red.to(device)`: Mueve la nueva red al mismo dispositivo que la original."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

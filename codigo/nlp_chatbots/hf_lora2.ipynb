{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sonder-art/automl_o24/blob/main/codigo/nlp_chatbots/hf_lora2.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# LoRA Finetuning with HuggingFace Transformers\n",
    "\n",
    "This notebook provides a comprehensive implementation of LoRA finetuning for HuggingFace language models. The code is modular, easily configurable, and supports both GPU (CUDA) and CPU execution.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "2. [Data Pipeline](#Data-Pipeline)\n",
    "3. [Model Architecture](#Model-Architecture)\n",
    "4. [Training Configuration](#Training-Configuration)\n",
    "5. [Training Features](#Training-Features)\n",
    "6. [Evaluation and Visualization](#Evaluation-and-Visualization)\n",
    "7. [Testing](#Testing)\n",
    "8. [Full Training Example](#Full-Training-Example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/uumami/.local/lib/python3.10/site-packages (4.44.0)\n",
      "Requirement already satisfied: datasets in /home/uumami/.local/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: accelerate in /home/uumami/.local/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/uumami/.local/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: loralib in /home/uumami/.local/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: torch in /home/uumami/.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/uumami/.local/lib/python3.10/site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/uumami/.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: requests in /home/uumami/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/uumami/.local/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/uumami/.local/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/uumami/.local/lib/python3.10/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: filelock in /home/uumami/.local/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/uumami/.local/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/uumami/.local/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/uumami/.local/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: aiohttp in /home/uumami/.local/lib/python3.10/site-packages (from datasets) (3.10.3)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /home/uumami/.local/lib/python3.10/site-packages (from datasets) (2024.6.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/uumami/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/uumami/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/uumami/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/uumami/.local/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pandas in /home/uumami/.local/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: psutil in /home/uumami/.local/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: jinja2 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: networkx in /home/uumami/.local/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/uumami/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/uumami/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/uumami/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/uumami/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/uumami/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/uumami/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/uumami/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/uumami/.local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/uumami/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.3.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/uumami/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/uumami/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/uumami/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/uumami/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages if not already installed\n",
    "%pip install transformers datasets accelerate bitsandbytes loralib torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import HfFolder, login\n",
    "from transformers import AutoModel\n",
    "\n",
    "def setup_huggingface(token):\n",
    "    \"\"\"Setup HuggingFace authentication and verify login\"\"\"\n",
    "    try:\n",
    "        # Set token and login\n",
    "        os.environ[\"HUGGINGFACE_TOKEN\"] = token\n",
    "        login(token=token)\n",
    "        \n",
    "        # Verify login by attempting to download a private model\n",
    "        # This will fail if not properly authenticated\n",
    "        test_model = AutoModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n",
    "        print(\"âœ“ Login successful - You can now access private models and datasets\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Login failed: {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/uumami/.cache/huggingface/token\n",
      "Login successful\n",
      "âœ“ Login successful - You can now access private models and datasets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Usage\n",
    "token = \"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\"  # Replace with your token from https://huggingface.co/settings/tokens\n",
    "setup_huggingface(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "- **CUDA Support**: Automatically uses GPU if available, with a fallback to CPU.\n",
    "- **Modular Architecture**: Functions and classes are designed for reuse.\n",
    "- **Error Handling and Logging**: Implemented throughout the code.\n",
    "- **Model and Data Agnostic**: Easily change models and datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Returns the available device (GPU/CPU).\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The device to use.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "device = get_device()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline\n",
    "\n",
    "We will create train/validation/test splits and preprocess the data with proper tokenization, dynamic padding, and input validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_dataset(\n",
    "    dataset_name: str,\n",
    "    split_ratios: Tuple[float, float, float] = (0.8, 0.1, 0.1),\n",
    "    **kwargs\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Loads and splits a dataset into train, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): The name of the dataset to load.\n",
    "        split_ratios (Tuple[float, float, float], optional): Ratios for train, val, test splits.\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: A dictionary with 'train', 'validation', and 'test' datasets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_name, **kwargs)\n",
    "        logger.info(f\"Loaded dataset {dataset_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset {dataset_name}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Assume the dataset has a 'train' split\n",
    "    train_val_test = dataset[\"train\"].train_test_split(\n",
    "        test_size=split_ratios[2],\n",
    "        seed=42\n",
    "    )\n",
    "    train_val = train_val_test[\"train\"].train_test_split(\n",
    "        test_size=split_ratios[1] / (split_ratios[0] + split_ratios[1]),\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_val[\"train\"],\n",
    "        \"validation\": train_val[\"test\"],\n",
    "        \"test\": train_val_test[\"test\"]\n",
    "    })\n",
    "    logger.info(\"Split dataset into train, validation, and test sets\")\n",
    "    return datasets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded dataset wikitext\n",
      "INFO:__main__:Split dataset into train, validation, and test sets\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "datasets = load_and_split_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    datasets: DatasetDict,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    text_column_name: str = \"text\",\n",
    "    block_size: int = 128\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Tokenizes and preprocesses the datasets.\n",
    "\n",
    "    Args:\n",
    "        datasets (DatasetDict): The dataset dictionary.\n",
    "        tokenizer (AutoTokenizer): The tokenizer to use.\n",
    "        text_column_name (str, optional): The column name containing the text.\n",
    "        block_size (int, optional): The block size for chunking the data.\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: The tokenized datasets.\n",
    "    \"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[text_column_name])\n",
    "\n",
    "    tokenized_datasets = datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=[text_column_name],\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\"\n",
    "    )\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[\"input_ids\"])\n",
    "        if total_length >= block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        load_from_cache_file=True,\n",
    "        desc=f\"Grouping texts into chunks of {block_size}\"\n",
    "    )\n",
    "\n",
    "    logger.info(\"Preprocessed datasets\")\n",
    "    return lm_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Preprocessed datasets\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # For GPT-2\n",
    "lm_datasets = preprocess_data(datasets, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "We select a base model and configure it for LoRA finetuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "\n",
    "def get_model(\n",
    "    model_name: str,\n",
    "    device: torch.device,\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.1,\n",
    "    target_modules: List[str] = [\"q_proj\", \"v_proj\"]\n",
    ") -> AutoModelForCausalLM:\n",
    "    \"\"\"\n",
    "    Loads a pre-trained model and prepares it for LoRA finetuning.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained model.\n",
    "        device (torch.device): The device to load the model on.\n",
    "        lora_r (int, optional): Rank of the LoRA matrices.\n",
    "        lora_alpha (int, optional): Alpha scaling parameter for LoRA.\n",
    "        lora_dropout (float, optional): Dropout probability for LoRA layers.\n",
    "        target_modules (List[str], optional): The modules to apply LoRA to.\n",
    "\n",
    "    Returns:\n",
    "        AutoModelForCausalLM: The model ready for finetuning.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        model.to(device)\n",
    "        logger.info(f\"Loaded model {model_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model {model_name}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Apply LoRA\n",
    "    for name, module in model.named_modules():\n",
    "        if any(target_module in name for target_module in target_modules):\n",
    "            lora.inject(module, r=lora_r, alpha=lora_alpha, dropout=lora_dropout)\n",
    "            logger.info(f\"Applied LoRA to {name}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73c1ed53cae41f9aab9a7981db4c60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3eb4c3c9b5486a80390a834f133223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb76f0a5cf34a6e8bf7fcd98c5ec6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error loading model meta-llama/Llama-3.2-1B-Instruct: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 3.94 GiB of which 19.38 MiB is free. Including non-PyTorch memory, this process has 3.91 GiB memory in use. Of the allocated memory 3.86 GiB is allocated by PyTorch, and 13.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uumami/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "model = get_model(\"meta-llama/Llama-3.2-1B-Instruct\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "We expose key hyperparameters for easy configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    learning_rate: float = 5e-5\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.1\n",
    "    num_train_epochs: int = 3\n",
    "    per_device_train_batch_size: int = 4\n",
    "    per_device_eval_batch_size: int = 4\n",
    "    evaluation_strategy: str = \"steps\"\n",
    "    eval_steps: int = 500\n",
    "    save_steps: int = 500\n",
    "    logging_steps: int = 100\n",
    "    output_dir: str = \"./results\"\n",
    "    seed: int = 42\n",
    "\n",
    "# Example usage:\n",
    "config = TrainingConfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Features\n",
    "\n",
    "We implement the loss function, metrics tracking, validation callbacks, and checkpointing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "    Computes perplexity and other metrics.\n",
    "\n",
    "    Args:\n",
    "        eval_preds (Tuple): Predictions and labels.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: The computed metrics.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    shift_logits = logits[..., :-1, :].reshape(-1, logits.shape[-1])\n",
    "    shift_labels = labels[..., 1:].reshape(-1)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(\n",
    "        torch.tensor(shift_logits),\n",
    "        torch.tensor(shift_labels)\n",
    "    )\n",
    "    perplexity = torch.exp(loss)\n",
    "    return {\"perplexity\": perplexity.item()}\n",
    "\n",
    "# No need to test this function separately as it will be used during evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_collator(tokenizer: AutoTokenizer):\n",
    "    \"\"\"\n",
    "    Returns a data collator for language modeling.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (AutoTokenizer): The tokenizer used.\n",
    "\n",
    "    Returns:\n",
    "        DataCollatorForLanguageModeling: The data collator.\n",
    "    \"\"\"\n",
    "    return DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False\n",
    "    )\n",
    "\n",
    "data_collator = get_data_collator(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "def train_model(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    lm_datasets: DatasetDict,\n",
    "    config: TrainingConfig\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model with the given configuration.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): The model to train.\n",
    "        tokenizer (AutoTokenizer): The tokenizer.\n",
    "        lm_datasets (DatasetDict): The tokenized datasets.\n",
    "        config (TrainingConfig): The training configuration.\n",
    "    \"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config.output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=config.num_train_epochs,\n",
    "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "        evaluation_strategy=config.evaluation_strategy,\n",
    "        learning_rate=config.learning_rate,\n",
    "        save_steps=config.save_steps,\n",
    "        eval_steps=config.eval_steps,\n",
    "        logging_steps=config.logging_steps,\n",
    "        seed=config.seed,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"],\n",
    "        eval_dataset=lm_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    logger.info(\"Training complete\")\n",
    "    return trainer\n",
    "\n",
    "# Example usage will be in the full training section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Visualization\n",
    "\n",
    "We evaluate the model before and after finetuning and visualize training progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    trainer: Trainer,\n",
    "    lm_datasets: DatasetDict,\n",
    "    split: str = \"test\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the specified dataset split.\n",
    "\n",
    "    Args:\n",
    "        trainer (Trainer): The trainer object.\n",
    "        lm_datasets (DatasetDict): The tokenized datasets.\n",
    "        split (str, optional): The dataset split to evaluate on.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Evaluation metrics.\n",
    "    \"\"\"\n",
    "    eval_results = trainer.evaluate(eval_dataset=lm_datasets[split])\n",
    "    logger.info(f\"Evaluation results on {split} set: {eval_results}\")\n",
    "    return eval_results\n",
    "\n",
    "# No separate test code needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(log_history: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Plots training metrics from the trainer's log history.\n",
    "\n",
    "    Args:\n",
    "        log_history (List[Dict[str, Any]]): The trainer's log history.\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    losses = []\n",
    "    eval_losses = []\n",
    "    perplexities = []\n",
    "    for log in log_history:\n",
    "        if \"loss\" in log:\n",
    "            steps.append(log[\"step\"])\n",
    "            losses.append(log[\"loss\"])\n",
    "        if \"eval_loss\" in log:\n",
    "            eval_losses.append(log[\"eval_loss\"])\n",
    "        if \"perplexity\" in log:\n",
    "            perplexities.append(log[\"perplexity\"])\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(steps, losses, label=\"Training Loss\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(eval_losses, label=\"Validation Loss\")\n",
    "    plt.plot(perplexities, label=\"Perplexity\")\n",
    "    plt.xlabel(\"Evaluation Steps\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# No separate test code needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "We perform model inference before and after finetuning and analyze performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = 50,\n",
    "    num_return_sequences: int = 1\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates text using the model based on the prompt.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): The language model.\n",
    "        tokenizer (AutoTokenizer): The tokenizer.\n",
    "        prompt (str): The text prompt.\n",
    "        max_length (int, optional): Maximum length of generated text.\n",
    "        num_return_sequences (int, optional): Number of sequences to generate.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Generated text sequences.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=60\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before finetuning: ['Once upon a time, the world had seen a fair, orderly and stable society, where everyone had a decent standard of living, the rich were cared for by their fellow citizens, and the poor themselves received adequate protection from cruel rulers.\\n\\n\\nToday']\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "pre_finetune_output = generate_text(model, tokenizer, \"Once upon a time\")\n",
    "print(\"Before finetuning:\", pre_finetune_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Example\n",
    "\n",
    "We bring everything together and run the full training pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded dataset wikitext\n",
      "INFO:__main__:Split dataset into train, validation, and test sets\n",
      "/home/uumami/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "INFO:__main__:Preprocessed datasets\n",
      "INFO:__main__:Using device: cuda\n",
      "INFO:__main__:Loaded model gpt2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38a5fa55b1345efba826cf18d57fcc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Evaluation results on test set: {'eval_loss': 4.238061428070068, 'eval_model_preparation_time': 0.0021, 'eval_runtime': 52.5668, 'eval_samples_per_second': 36.068, 'eval_steps_per_second': 4.509}\n",
      "/home/uumami/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b717ab2d6c034031b4dbebb248a2e041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.074, 'grad_norm': 14.423720359802246, 'learning_rate': 4.933047670058918e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 28\u001b[0m\n\u001b[1;32m     21\u001b[0m pre_eval_results \u001b[38;5;241m=\u001b[39m evaluate_model(\n\u001b[1;32m     22\u001b[0m     Trainer(model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer),\n\u001b[1;32m     23\u001b[0m     lm_datasets,\n\u001b[1;32m     24\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Post-finetuning evaluation\u001b[39;00m\n\u001b[1;32m     31\u001b[0m post_eval_results \u001b[38;5;241m=\u001b[39m evaluate_model(trainer, lm_datasets, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 45\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, tokenizer, lm_datasets, config)\u001b[0m\n\u001b[1;32m     18\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     19\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_dir,\n\u001b[1;32m     20\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     35\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     36\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)]\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 45\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2294\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2288\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2289\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2292\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2293\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2295\u001b[0m ):\n\u001b[1;32m   2296\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "datasets = load_and_split_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # For GPT-2\n",
    "lm_datasets = preprocess_data(datasets, tokenizer)\n",
    "\n",
    "# Load model\n",
    "device = get_device()\n",
    "model = get_model(\"gpt2\", device)\n",
    "\n",
    "# Training configuration\n",
    "config = TrainingConfig(\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,  # For quick example, set to higher for real training\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    output_dir=\"./gpt2-lora-finetuned\"\n",
    ")\n",
    "\n",
    "# Pre-finetuning evaluation\n",
    "pre_eval_results = evaluate_model(\n",
    "    Trainer(model=model, tokenizer=tokenizer),\n",
    "    lm_datasets,\n",
    "    split=\"test\"\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer = train_model(model, tokenizer, lm_datasets, config)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-finetuning evaluation\n",
    "post_eval_results = evaluate_model(trainer, lm_datasets, split=\"test\")\n",
    "\n",
    "# Visualization\n",
    "plot_metrics(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text after finetuning\n",
    "post_finetune_output = generate_text(model, tokenizer, \"Once upon a time\")\n",
    "print(\"After finetuning:\", post_finetune_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

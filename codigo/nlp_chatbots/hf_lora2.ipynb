{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmQ-zgpBMGJc"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sonder-art/automl_o24/blob/main/codigo/nlp_chatbots/hf_lora2.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "PPgvIWKNMGJd"
      },
      "source": [
        "# LoRA Finetuning with HuggingFace Transformers\n",
        "\n",
        "This notebook provides a comprehensive implementation of LoRA finetuning for HuggingFace language models. The code is modular, easily configurable, and supports both GPU (CUDA) and CPU execution.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Prerequisites](#Prerequisites)\n",
        "2. [Data Pipeline](#Data-Pipeline)\n",
        "3. [Model Architecture](#Model-Architecture)\n",
        "4. [Training Configuration](#Training-Configuration)\n",
        "5. [Training Features](#Training-Features)\n",
        "6. [Evaluation and Visualization](#Evaluation-and-Visualization)\n",
        "7. [Testing](#Testing)\n",
        "8. [Full Training Example](#Full-Training-Example)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPbFaExAMGJe",
        "outputId": "f3a1fda6-00d9-40bd-af7e-4867fbc61789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages if not already installed\n",
        "%pip install transformers datasets accelerate bitsandbytes loralib torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tSeKvx17MGJg"
      },
      "outputs": [],
      "source": [
        "%pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U transformers peft bitsandbytes accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmUes6e6UxhF",
        "outputId": "9cb3a7c6-0432-4c2c-9557-839ade21970d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ivk3KO_nMGJi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import HfFolder, login\n",
        "from transformers import AutoModel\n",
        "\n",
        "def setup_huggingface(token):\n",
        "    \"\"\"Setup HuggingFace authentication and verify login\"\"\"\n",
        "    try:\n",
        "        # Set token and login\n",
        "        os.environ[\"HUGGINGFACE_TOKEN\"] = token\n",
        "        login(token=token)\n",
        "\n",
        "        # Verify login by attempting to download a private model\n",
        "        # This will fail if not properly authenticated\n",
        "        test_model = AutoModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n",
        "        print(\"✓ Login successful - You can now access private models and datasets\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Login failed: {str(e)}\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEQCSqY4MGJj",
        "outputId": "14ef720d-6aca-4e2d-edeb-ff29bf667a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Login successful - You can now access private models and datasets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "# Usage\n",
        "token = \"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\"  # Replace with your token from https://huggingface.co/settings/tokens\n",
        "setup_huggingface(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "E8w0wy0gMGJk"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "- **CUDA Support**: Automatically uses GPU if available, with a fallback to CPU.\n",
        "- **Modular Architecture**: Functions and classes are designed for reuse.\n",
        "- **Error Handling and Logging**: Implemented throughout the code.\n",
        "- **Model and Data Agnostic**: Easily change models and datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jcTHqzKsMGJk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import numpy as np\n",
        "from typing import Any, Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "93AnwrHeMGJk"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Returns the available device (GPU/CPU).\n",
        "\n",
        "    Returns:\n",
        "        torch.device: The device to use.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "    return device\n",
        "\n",
        "device = get_device()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S9XcmZKMGJm"
      },
      "source": [
        "## Data Pipeline\n",
        "\n",
        "We will create train/validation/test splits and preprocess the data with proper tokenization, dynamic padding, and input validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dFfY3q-NMGJm"
      },
      "outputs": [],
      "source": [
        "def load_and_split_dataset(\n",
        "    dataset_name: str,\n",
        "    split_ratios: Tuple[float, float, float] = (0.8, 0.1, 0.1),\n",
        "    **kwargs\n",
        ") -> DatasetDict:\n",
        "    \"\"\"\n",
        "    Loads and splits a dataset into train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): The name of the dataset to load.\n",
        "        split_ratios (Tuple[float, float, float], optional): Ratios for train, val, test splits.\n",
        "\n",
        "    Returns:\n",
        "        DatasetDict: A dictionary with 'train', 'validation', and 'test' datasets.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dataset = load_dataset(dataset_name, **kwargs)\n",
        "        logger.info(f\"Loaded dataset {dataset_name}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading dataset {dataset_name}: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Assume the dataset has a 'train' split\n",
        "    train_val_test = dataset[\"train\"].train_test_split(\n",
        "        test_size=split_ratios[2],\n",
        "        seed=42\n",
        "    )\n",
        "    train_val = train_val_test[\"train\"].train_test_split(\n",
        "        test_size=split_ratios[1] / (split_ratios[0] + split_ratios[1]),\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    datasets = DatasetDict({\n",
        "        \"train\": train_val[\"train\"],\n",
        "        \"validation\": train_val[\"test\"],\n",
        "        \"test\": train_val_test[\"test\"]\n",
        "    })\n",
        "    logger.info(\"Split dataset into train, validation, and test sets\")\n",
        "    return datasets\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "klSsgc5BMGJm"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "datasets = load_and_split_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AXIuAN4JMGJn"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(\n",
        "    datasets: DatasetDict,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    text_column_name: str = \"text\",\n",
        "    block_size: int = 128\n",
        ") -> DatasetDict:\n",
        "    \"\"\"\n",
        "    Tokenizes and preprocesses the datasets.\n",
        "\n",
        "    Args:\n",
        "        datasets (DatasetDict): The dataset dictionary.\n",
        "        tokenizer (AutoTokenizer): The tokenizer to use.\n",
        "        text_column_name (str, optional): The column name containing the text.\n",
        "        block_size (int, optional): The block size for chunking the data.\n",
        "\n",
        "    Returns:\n",
        "        DatasetDict: The tokenized datasets.\n",
        "    \"\"\"\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[text_column_name])\n",
        "\n",
        "    tokenized_datasets = datasets.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        num_proc=4,\n",
        "        remove_columns=[text_column_name],\n",
        "        load_from_cache_file=True,\n",
        "        desc=\"Running tokenizer on dataset\"\n",
        "    )\n",
        "\n",
        "    def group_texts(examples):\n",
        "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "        total_length = len(concatenated_examples[\"input_ids\"])\n",
        "        if total_length >= block_size:\n",
        "            total_length = (total_length // block_size) * block_size\n",
        "        result = {\n",
        "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "        return result\n",
        "\n",
        "    lm_datasets = tokenized_datasets.map(\n",
        "        group_texts,\n",
        "        batched=True,\n",
        "        num_proc=4,\n",
        "        load_from_cache_file=True,\n",
        "        desc=f\"Grouping texts into chunks of {block_size}\"\n",
        "    )\n",
        "\n",
        "    logger.info(\"Preprocessed datasets\")\n",
        "    return lm_datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zCDorVLMMGJo"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "lm_datasets = preprocess_data(datasets, tokenizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF-V4VH8MGJo"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "We select a base model and configure it for LoRA finetuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m2r_t0LpMGJo"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import torch\n",
        "import logging\n",
        "from typing import List\n",
        "from datasets import DatasetDict\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_model(\n",
        "    model_name: str,\n",
        "    lora_r: int = 8,\n",
        "    lora_alpha: int = 16,\n",
        "    lora_dropout: float = 0.1,\n",
        "    target_modules: List[str] = [\"q_proj\", \"v_proj\"]\n",
        ") -> AutoModelForCausalLM:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained model and prepares it for LoRA finetuning using PEFT.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Configure quantization\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            llm_int8_threshold=6.0,\n",
        "            llm_int8_has_fp16_weight=False,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "\n",
        "        # Load base model with quantization config\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            use_cache=False  # Disable cache to save memory\n",
        "        )\n",
        "\n",
        "        # Prepare model for k-bit training\n",
        "        model = prepare_model_for_kbit_training(\n",
        "            model,\n",
        "            use_gradient_checkpointing=True\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Loaded and prepared base model {model_name}\")\n",
        "\n",
        "        # Define LoRA Config\n",
        "        peft_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=lora_r,\n",
        "            lora_alpha=lora_alpha,\n",
        "            lora_dropout=lora_dropout,\n",
        "            target_modules=target_modules,\n",
        "            bias=\"none\"\n",
        "        )\n",
        "\n",
        "        # Create PEFT model\n",
        "        model = get_peft_model(model, peft_config)\n",
        "\n",
        "        # Enable gradient checkpointing for memory efficiency\n",
        "        model.gradient_checkpointing_enable()\n",
        "        model.enable_input_require_grads()\n",
        "\n",
        "        # Print trainable parameters\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        return model\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error setting up model with LoRA: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcldRVhtMGJp",
        "outputId": "d7096ec3-340f-4a36-e80a-31d9c0c92eb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "model = get_model(\"meta-llama/Llama-3.2-1B-Instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW22rD8zMGJp"
      },
      "source": [
        "## Training Configuration\n",
        "\n",
        "We expose key hyperparameters for easy configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RmV5NGz3MGJp"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    learning_rate: float = 5e-5\n",
        "    lora_r: int = 8\n",
        "    lora_alpha: int = 16\n",
        "    lora_dropout: float = 0.1\n",
        "    num_train_epochs: int = 3\n",
        "    per_device_train_batch_size: int = 4\n",
        "    per_device_eval_batch_size: int = 4\n",
        "    evaluation_strategy: str = \"steps\"\n",
        "    eval_steps: int = 500\n",
        "    save_steps: int = 500\n",
        "    logging_steps: int = 100\n",
        "    output_dir: str = \"./results\"\n",
        "    seed: int = 42\n",
        "    gradient_accumulation_steps: int = 4  # Added this line\n",
        "\n",
        "# Example usage:\n",
        "config = TrainingConfig()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWaAfBXtMGJq"
      },
      "source": [
        "## Training Features\n",
        "\n",
        "We implement the loss function, metrics tracking, validation callbacks, and checkpointing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Rg-AdJetMGJq"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"\n",
        "    Computes perplexity and other metrics.\n",
        "\n",
        "    Args:\n",
        "        eval_preds (Tuple): Predictions and labels.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: The computed metrics.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    shift_logits = logits[..., :-1, :].reshape(-1, logits.shape[-1])\n",
        "    shift_labels = labels[..., 1:].reshape(-1)\n",
        "    loss_fct = torch.nn.CrossEntropyLoss()\n",
        "    loss = loss_fct(\n",
        "        torch.tensor(shift_logits),\n",
        "        torch.tensor(shift_labels)\n",
        "    )\n",
        "    perplexity = torch.exp(loss)\n",
        "    return {\"perplexity\": perplexity.item()}\n",
        "\n",
        "# No need to test this function separately as it will be used during evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2-8SwTIlMGJq"
      },
      "outputs": [],
      "source": [
        "def get_data_collator(tokenizer: AutoTokenizer):\n",
        "    \"\"\"\n",
        "    Returns a data collator for language modeling.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (AutoTokenizer): The tokenizer used.\n",
        "\n",
        "    Returns:\n",
        "        DataCollatorForLanguageModeling: The data collator.\n",
        "    \"\"\"\n",
        "    return DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False\n",
        "    )\n",
        "\n",
        "data_collator = get_data_collator(tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import logging\n",
        "import math\n",
        "import numpy as np\n",
        "import wandb\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, Optional, List\n",
        "import transformers\n",
        "from transformers import (\n",
        "    Trainer, AutoModelForCausalLM, TrainingArguments,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import DatasetDict\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class WandbMetricsCallback(transformers.TrainerCallback):\n",
        "    \"\"\"Custom callback for logging well-organized metrics to W&B\"\"\"\n",
        "    def __init__(self, total_train_steps: int):\n",
        "        self.total_train_steps = total_train_steps\n",
        "\n",
        "    def format_metric_name(self, name: str) -> str:\n",
        "        \"\"\"Format metric names for better organization in W&B\"\"\"\n",
        "        categories = {\n",
        "            \"loss\": \"metrics/loss/\",\n",
        "            \"learning_rate\": \"training/progress/\",\n",
        "            \"epoch\": \"training/progress/\",\n",
        "            \"eval_loss\": \"metrics/loss/\",\n",
        "            \"eval_perplexity\": \"metrics/perplexity/\",\n",
        "            \"train_runtime\": \"time/\",\n",
        "            \"train_samples_per_second\": \"performance/\",\n",
        "            \"train_steps_per_second\": \"performance/\",\n",
        "        }\n",
        "\n",
        "        for key, prefix in categories.items():\n",
        "            if key in name:\n",
        "                return prefix + name.replace(\"eval_\", \"validation_\")\n",
        "        return \"other/\" + name\n",
        "\n",
        "    def on_log(self, args, state, control, logs={}, **kwargs):\n",
        "        \"\"\"Log metrics with organized naming and create plots\"\"\"\n",
        "        formatted_logs = {}\n",
        "\n",
        "        # Calculate epoch as float for smoother plotting\n",
        "        if 'step' in logs:\n",
        "            current_step = logs['step']\n",
        "            epoch = current_step / self.total_train_steps\n",
        "            formatted_logs[\"training/progress/epoch\"] = epoch\n",
        "\n",
        "        # Process metrics\n",
        "        for key, value in logs.items():\n",
        "            formatted_name = self.format_metric_name(key)\n",
        "            formatted_logs[formatted_name] = value\n",
        "\n",
        "            # Add perplexity calculations\n",
        "            if 'loss' in key:\n",
        "                perplexity = np.exp(value)\n",
        "                if 'eval' in key:\n",
        "                    formatted_logs[\"metrics/perplexity/validation\"] = perplexity\n",
        "                else:\n",
        "                    formatted_logs[\"metrics/perplexity/train\"] = perplexity\n",
        "\n",
        "        # Create evaluation plots\n",
        "        if 'eval_loss' in logs:\n",
        "            history = state.log_history\n",
        "            steps = [x.get('step', 0) for x in history if 'step' in x]\n",
        "            epochs = [s / self.total_train_steps for s in steps]\n",
        "\n",
        "            # Extract metrics\n",
        "            train_metrics = [(i, x['loss']) for i, x in enumerate(history) if 'loss' in x and 'eval' not in x]\n",
        "            eval_metrics = [(i, x['eval_loss']) for i, x in enumerate(history) if 'eval_loss' in x]\n",
        "\n",
        "            # Create training progress plot\n",
        "            fig = make_subplots(\n",
        "                rows=2, cols=1,\n",
        "                subplot_titles=(\n",
        "                    'Training and Validation Loss over Time',\n",
        "                    'Training and Validation Perplexity over Time'\n",
        "                ),\n",
        "                vertical_spacing=0.15\n",
        "            )\n",
        "\n",
        "            # Add loss traces\n",
        "            if train_metrics:\n",
        "                train_x, train_y = zip(*train_metrics)\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(x=epochs[:len(train_x)], y=train_y,\n",
        "                              name='Training Loss',\n",
        "                              line=dict(color='#0000FF', width=2)),\n",
        "                    row=1, col=1\n",
        "                )\n",
        "\n",
        "            if eval_metrics:\n",
        "                eval_x, eval_y = zip(*eval_metrics)\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(x=epochs[:len(eval_x)], y=eval_y,\n",
        "                              name='Validation Loss',\n",
        "                              line=dict(color='#FF0000', width=2)),\n",
        "                    row=1, col=1\n",
        "                )\n",
        "\n",
        "            # Add perplexity traces\n",
        "            if train_metrics:\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(x=epochs[:len(train_x)],\n",
        "                              y=[np.exp(y) for y in train_y],\n",
        "                              name='Training Perplexity',\n",
        "                              line=dict(color='#0000FF', width=2, dash='dot')),\n",
        "                    row=2, col=1\n",
        "                )\n",
        "\n",
        "            if eval_metrics:\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(x=epochs[:len(eval_x)],\n",
        "                              y=[np.exp(y) for y in eval_y],\n",
        "                              name='Validation Perplexity',\n",
        "                              line=dict(color='#FF0000', width=2, dash='dot')),\n",
        "                    row=2, col=1\n",
        "                )\n",
        "\n",
        "            # Update layout\n",
        "            fig.update_layout(\n",
        "                height=800,\n",
        "                title_text=\"Model Training Progress\",\n",
        "                template=\"plotly_white\",\n",
        "                showlegend=True,\n",
        "                legend=dict(\n",
        "                    yanchor=\"bottom\",\n",
        "                    y=1.02,\n",
        "                    xanchor=\"right\",\n",
        "                    x=1\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Update axes\n",
        "            for i in range(1, 3):\n",
        "                fig.update_xaxes(title_text=\"Epochs\", row=i, col=1,\n",
        "                               gridcolor='lightgray')\n",
        "\n",
        "            fig.update_yaxes(title_text=\"Loss\", row=1, col=1,\n",
        "                          gridcolor='lightgray', zeroline=True)\n",
        "            fig.update_yaxes(title_text=\"Perplexity\", row=2, col=1,\n",
        "                          gridcolor='lightgray', zeroline=True)\n",
        "\n",
        "            formatted_logs[\"visualizations/training_progress\"] = wandb.Plotly(fig)\n",
        "\n",
        "        wandb.log(formatted_logs, step=state.global_step)\n",
        "\n",
        "def compute_metrics_with_perplexity(eval_pred):\n",
        "    \"\"\"Enhanced metric computation including perplexity\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    metrics = compute_metrics(eval_pred)  # Your existing compute_metrics\n",
        "\n",
        "    # Add perplexity\n",
        "    loss = np.mean(predictions[predictions != -100])\n",
        "    metrics[\"perplexity\"] = math.exp(loss)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def train_model(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer,\n",
        "    lm_datasets: DatasetDict,\n",
        "    config,\n",
        "    project_name: str = \"language-model-training\",\n",
        "    run_name: Optional[str] = None,\n",
        "    tags: Optional[list] = None,\n",
        "    notes: Optional[str] = None,\n",
        "    log_plots: bool = True  # New parameter\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains the model with enhanced W&B logging and organization.\n",
        "    \"\"\"\n",
        "    # Initialize W&B with enhanced configuration\n",
        "    # Calculate total steps for epoch tracking\n",
        "    total_train_steps = (\n",
        "        len(lm_datasets[\"train\"])\n",
        "        // (config.per_device_train_batch_size * config.gradient_accumulation_steps)\n",
        "        * config.num_train_epochs\n",
        "    )\n",
        "    wandb.init(\n",
        "        project=project_name,\n",
        "        name=run_name,\n",
        "        tags=tags,\n",
        "        notes=notes,\n",
        "        config={\n",
        "            \"model_name\": model.config.name_or_path,\n",
        "            \"train_samples\": len(lm_datasets[\"train\"]),\n",
        "            \"validation_samples\": len(lm_datasets[\"validation\"]),\n",
        "            \"total_train_steps\": total_train_steps,\n",
        "            **config.__dict__\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=config.output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=config.num_train_epochs,\n",
        "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
        "        eval_strategy=\"steps\",\n",
        "        learning_rate=config.learning_rate,\n",
        "        save_steps=config.save_steps,\n",
        "        eval_steps=config.eval_steps,\n",
        "        logging_steps=config.logging_steps,\n",
        "        seed=config.seed,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        fp16=True,\n",
        "        gradient_checkpointing=True,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm=0.3,\n",
        "        report_to=\"wandb\"\n",
        "    )\n",
        "\n",
        "# Add a custom callback for plotting\n",
        "    class MetricsPlottingCallback(transformers.TrainerCallback):\n",
        "        def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "            if not log_plots or not metrics:\n",
        "                return\n",
        "\n",
        "            # Create loss plot\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "            # Loss plot\n",
        "            history = state.log_history\n",
        "            train_steps = [x['step'] for x in history if 'loss' in x]\n",
        "            train_losses = [x['loss'] for x in history if 'loss' in x]\n",
        "            eval_steps = [x['step'] for x in history if 'eval_loss' in x]\n",
        "            eval_losses = [x['eval_loss'] for x in history if 'eval_loss' in x]\n",
        "\n",
        "            ax1.plot(train_steps, train_losses, label='Training Loss')\n",
        "            if eval_losses:\n",
        "                ax1.plot(eval_steps, eval_losses, label='Validation Loss')\n",
        "            ax1.set_xlabel('Steps')\n",
        "            ax1.set_ylabel('Loss')\n",
        "            ax1.legend()\n",
        "            ax1.set_title('Training and Validation Loss')\n",
        "\n",
        "            # Perplexity plot\n",
        "            train_perplexity = [math.exp(x) for x in train_losses]\n",
        "            eval_perplexity = [math.exp(x) for x in eval_losses] if eval_losses else []\n",
        "\n",
        "            ax2.plot(train_steps, train_perplexity, label='Training Perplexity')\n",
        "            if eval_perplexity:\n",
        "                ax2.plot(eval_steps, eval_perplexity, label='Validation Perplexity')\n",
        "            ax2.set_xlabel('Steps')\n",
        "            ax2.set_ylabel('Perplexity')\n",
        "            ax2.legend()\n",
        "            ax2.set_title('Training and Validation Perplexity')\n",
        "\n",
        "            # Log to wandb\n",
        "            wandb.log({\n",
        "                \"training/loss_and_perplexity\": wandb.Image(fig)\n",
        "            }, step=state.global_step)\n",
        "\n",
        "            plt.close(fig)\n",
        "\n",
        "    # Setup trainer with new callback\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=lm_datasets[\"train\"],\n",
        "        eval_dataset=lm_datasets[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics_with_perplexity,\n",
        "        callbacks=[\n",
        "            EarlyStoppingCallback(early_stopping_patience=3),\n",
        "            WandbMetricsCallback(total_train_steps=total_train_steps)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Train and log final metrics\n",
        "    trainer.train()\n",
        "    logger.info(\"Training complete\")\n",
        "\n",
        "    # Close W&B run\n",
        "    wandb.finish()\n",
        "\n",
        "    return trainer\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    trainer: Trainer,\n",
        "    lm_datasets: DatasetDict,\n",
        "    split: str = \"test\",\n",
        "    log_to_wandb: bool = True,\n",
        "    batch_size: int = 32\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the specified dataset split with efficient memory management.\n",
        "    Logs results to wandb if requested.\n",
        "    \"\"\"\n",
        "    print_gpu_memory(f\"Before {split} evaluation:\")\n",
        "    try:\n",
        "        model = trainer.model\n",
        "        model.eval()\n",
        "\n",
        "        # Create DataLoader with smaller batch size\n",
        "        eval_dataset = lm_datasets[split]\n",
        "        eval_dataloader = DataLoader(\n",
        "            eval_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=trainer.data_collator\n",
        "        )\n",
        "\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(eval_dataloader, desc=f\"Evaluating {split} set\"):\n",
        "                clean_memory_preserve_model()\n",
        "\n",
        "                # Move batch to device\n",
        "                batch = {k: v.to(model.device) if hasattr(v, 'to') else v\n",
        "                        for k, v in batch.items()}\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                # Accumulate loss\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "                # Clear batch from GPU safely\n",
        "                for k in list(batch.keys()):\n",
        "                    if hasattr(batch[k], 'to'):\n",
        "                        batch[k] = batch[k].cpu()\n",
        "                        del batch[k]\n",
        "\n",
        "                del outputs\n",
        "                del loss\n",
        "                del batch\n",
        "                clean_memory_preserve_model()\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate metrics\n",
        "        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
        "\n",
        "        eval_results = {\n",
        "            \"eval_loss\": avg_loss,\n",
        "            \"eval_perplexity\": perplexity\n",
        "        }\n",
        "\n",
        "        # Log to wandb if requested\n",
        "        if log_to_wandb and wandb.run is not None:\n",
        "            metrics_dict = {\n",
        "                f\"{split}/loss\": avg_loss,\n",
        "                f\"{split}/perplexity\": perplexity,\n",
        "            }\n",
        "\n",
        "            # Create and log plot\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "            # Loss plot\n",
        "            ax1.bar(['Loss'], [avg_loss])\n",
        "            ax1.set_title(f'{split.capitalize()} Loss')\n",
        "\n",
        "            # Perplexity plot\n",
        "            ax2.bar(['Perplexity'], [perplexity])\n",
        "            ax2.set_title(f'{split.capitalize()} Perplexity')\n",
        "\n",
        "            # Log plot to wandb\n",
        "            metrics_dict[f\"{split}/metrics_plot\"] = wandb.Image(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "            wandb.log(metrics_dict)\n",
        "\n",
        "        logger.info(f\"Evaluation results on {split} set: {eval_results}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during evaluation: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        clean_memory_preserve_model()\n",
        "        print_gpu_memory(f\"After {split} evaluation:\")\n",
        "\n",
        "    return eval_results"
      ],
      "metadata": {
        "id": "gTKhEfjynAQs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5uTDFn6MGJr"
      },
      "source": [
        "## Evaluation and Visualization\n",
        "\n",
        "We evaluate the model before and after finetuning and visualize training progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VJKinzOqMGJr"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(log_history: List[Dict[str, Any]]):\n",
        "    \"\"\"\n",
        "    Plots training metrics from the trainer's log history.\n",
        "\n",
        "    Args:\n",
        "        log_history (List[Dict[str, Any]]): The trainer's log history.\n",
        "    \"\"\"\n",
        "    steps = []\n",
        "    losses = []\n",
        "    eval_losses = []\n",
        "    perplexities = []\n",
        "    for log in log_history:\n",
        "        if \"loss\" in log:\n",
        "            steps.append(log[\"step\"])\n",
        "            losses.append(log[\"loss\"])\n",
        "        if \"eval_loss\" in log:\n",
        "            eval_losses.append(log[\"eval_loss\"])\n",
        "        if \"perplexity\" in log:\n",
        "            perplexities.append(log[\"perplexity\"])\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(steps, losses, label=\"Training Loss\")\n",
        "    plt.xlabel(\"Steps\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(eval_losses, label=\"Validation Loss\")\n",
        "    plt.plot(perplexities, label=\"Perplexity\")\n",
        "    plt.xlabel(\"Evaluation Steps\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# No separate test code needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTjILvKpMGJr"
      },
      "source": [
        "## Testing\n",
        "\n",
        "We perform model inference before and after finetuning and analyze performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZT22pqEOMGJs"
      },
      "outputs": [],
      "source": [
        "def generate_text(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    prompt: str,\n",
        "    max_length: int = 50,\n",
        "    num_return_sequences: int = 1\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Generates text using the model based on the prompt.\n",
        "\n",
        "    Args:\n",
        "        model (AutoModelForCausalLM): The language model.\n",
        "        tokenizer (AutoTokenizer): The tokenizer.\n",
        "        prompt (str): The text prompt.\n",
        "        max_length (int, optional): Maximum length of generated text.\n",
        "        num_return_sequences (int, optional): Number of sequences to generate.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: Generated text sequences.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        top_k=60\n",
        "    )\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beZadw11MGJs",
        "outputId": "16c5bb8d-ae23-4f78-c190-a6e44bbdcd05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before finetuning: ['Once upon a time, there lived a young girl named Sophia. She was a curious and adventurous soul, always eager to explore the world around her. One day, while wandering through the woods, Sophia stumbled upon a hidden clearing. In the center']\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "pre_finetune_output = generate_text(model, tokenizer, \"Once upon a time\")\n",
        "print(\"Before finetuning:\", pre_finetune_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ByvvXaDMGJs"
      },
      "source": [
        "## Full Training Example\n",
        "\n",
        "We bring everything together and run the full training pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def clean_memory():\n",
        "    \"\"\"\n",
        "    Cleans up GPU memory by deleting models and clearing CUDA cache.\n",
        "    \"\"\"\n",
        "    # Delete all global variables that might be models\n",
        "    for obj in list(globals()):\n",
        "        if isinstance(globals()[obj], torch.nn.Module):\n",
        "            del globals()[obj]\n",
        "\n",
        "    # Clear CUDA cache\n",
        "    if torch.cuda.is_available():\n",
        "        # Empty CUDA cache\n",
        "        torch.cuda.empty_cache()\n",
        "        # Clear memory allocations\n",
        "        torch.cuda.memory.empty_cache()\n",
        "        # Reset peak memory stats\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # Run garbage collector\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"Memory cleaned successfully!\")\n",
        "\n",
        "# Function to check GPU memory usage\n",
        "def print_gpu_memory(message):\n",
        "    \"\"\"\n",
        "    Prints current GPU memory usage.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Current GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "        print(f\"Maximum GPU memory allocated: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n",
        "        print(f\"Current GPU memory cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from transformers import Trainer\n",
        "from datasets import DatasetDict\n",
        "import logging\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def clean_memory_preserve_model():\n",
        "    \"\"\"\n",
        "    Cleans up GPU memory while preserving the model.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.memory.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def print_gpu_memory(message=\"GPU Memory Usage\"):\n",
        "    \"\"\"\n",
        "    Prints current GPU memory usage.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"\\n{message}\")\n",
        "        print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "        print(f\"Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
        "\n",
        "def evaluate_with_memory_tracking(trainer, lm_datasets, split=\"test\"):\n",
        "    \"\"\"\n",
        "    Wrapper function to track memory usage during evaluation.\n",
        "    \"\"\"\n",
        "    print_gpu_memory(\"Before evaluation:\")\n",
        "    try:\n",
        "        results = evaluate_model(\n",
        "    trainer,\n",
        "    lm_datasets,\n",
        "    split=\"test\",\n",
        "    log_to_wandb=True\n",
        "    )\n",
        "    finally:\n",
        "        clean_memory_preserve_model()\n",
        "        print_gpu_memory(\"After evaluation:\")\n",
        "    return results\n",
        "clean_memory_preserve_model()"
      ],
      "metadata": {
        "id": "WZaicGVPXAUi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# First, if you have a specific model variable you want to delete:\n",
        "if 'model' in globals():\n",
        "    del model\n",
        "\n",
        "# Then run the cleanup\n",
        "clean_memory()\n",
        "\n",
        "# Optionally, check memory usage\n",
        "print_gpu_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCIbdDgJXGae",
        "outputId": "a6f25628-3f84-46d2-e0c0-4545ec35d298"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory cleaned successfully!\n",
            "Current GPU memory allocated: 1946.33 MB\n",
            "Maximum GPU memory allocated: 1946.33 MB\n",
            "Current GPU memory cached: 2440.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F-dZ-3YMGJs",
        "outputId": "54869411-e19e-4402-b9cc-bd86effa83e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-4f9e96c3938b>:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  Trainer(model=model, tokenizer=tokenizer),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\n",
            "\n",
            "Before test evaluation:\n",
            "Allocated: 3878.03 MB\n",
            "Cached: 4496.00 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating test set:   0%|          | 0/62 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Evaluating test set: 100%|██████████| 62/62 [02:12<00:00,  2.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After test evaluation:\n",
            "Allocated: 3884.05 MB\n",
            "Cached: 4506.00 MB\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data\n",
        "MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "datasets = load_and_split_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "lm_datasets = preprocess_data(datasets, tokenizer)\n",
        "\n",
        "# Load model\n",
        "device = get_device()\n",
        "model = get_model(MODEL)\n",
        "\n",
        "pre_eval_results = evaluate_model(\n",
        "    Trainer(model=model, tokenizer=tokenizer),\n",
        "    lm_datasets,\n",
        "    split=\"test\",\n",
        "    log_to_wandb=True  # This will log as test/loss and test/perplexity\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "config = TrainingConfig(\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=.01,  # For quick example, set to higher for real training\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    output_dir=f\"./{MODEL}-lora-finetuned\"\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer = train_model(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    lm_datasets=lm_datasets,\n",
        "    config=config,\n",
        "    project_name='llama_3_1_1B_complete',\n",
        "    run_name='experiment_test_complete',\n",
        "    tags=[\"llama_3_1_1B\", \"wiki2raw\"],\n",
        "    notes=\"Training with enhanced learning rate\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683,
          "referenced_widgets": [
            "cb56a516836042029ca0e30e71a8bcd2",
            "f3f3ea508b964fcc8fd6a06f490dc584",
            "9864f2559da0451c88fcc5053cd3660d",
            "6e43e9eeb6d04567beb777dbae128554",
            "bf05edfd78664dc8836e2e1dcf665163",
            "c526f3e156e14a4783e32f610f14600f",
            "41b8256f02c24fd9a368afdd75995eb2",
            "14b37e1b469544bcaa42d3470f0d2379"
          ]
        },
        "id": "5SQE9f1gWGIk",
        "outputId": "58773b4d-e556-40c9-8db4-bc87f4ab2ad5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241126_003457-yeo3mt5w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sonder-art/llama_3_1_1B_complete/runs/yeo3mt5w' target=\"_blank\">experiment_test_complete</a></strong> to <a href='https://wandb.ai/sonder-art/llama_3_1_1B_complete' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sonder-art/llama_3_1_1B_complete' target=\"_blank\">https://wandb.ai/sonder-art/llama_3_1_1B_complete</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sonder-art/llama_3_1_1B_complete/runs/yeo3mt5w' target=\"_blank\">https://wandb.ai/sonder-art/llama_3_1_1B_complete/runs/yeo3mt5w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-de3adb760e9f>:269: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:20, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.023 MB of 0.023 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb56a516836042029ca0e30e71a8bcd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>metrics/loss/train_loss</td><td>▁</td></tr><tr><td>metrics/perplexity/train</td><td>▁</td></tr><tr><td>other/total_flos</td><td>▁</td></tr><tr><td>performance/train_samples_per_second</td><td>▁</td></tr><tr><td>performance/train_steps_per_second</td><td>▁</td></tr><tr><td>time/train_runtime</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>training/progress/epoch</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>metrics/loss/train_loss</td><td>3.878</td></tr><tr><td>metrics/perplexity/train</td><td>48.32752</td></tr><tr><td>other/total_flos</td><td>143621860884480.0</td></tr><tr><td>performance/train_samples_per_second</td><td>5.138</td></tr><tr><td>performance/train_steps_per_second</td><td>0.1</td></tr><tr><td>time/train_runtime</td><td>30.0406</td></tr><tr><td>total_flos</td><td>143621860884480.0</td></tr><tr><td>train/epoch</td><td>0.01244</td></tr><tr><td>train/global_step</td><td>3</td></tr><tr><td>train_loss</td><td>3.878</td></tr><tr><td>train_runtime</td><td>30.0406</td></tr><tr><td>train_samples_per_second</td><td>5.138</td></tr><tr><td>train_steps_per_second</td><td>0.1</td></tr><tr><td>training/progress/epoch</td><td>0.01244</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_test_complete</strong> at: <a href='https://wandb.ai/sonder-art/llama_3_1_1B_complete/runs/yeo3mt5w' target=\"_blank\">https://wandb.ai/sonder-art/llama_3_1_1B_complete/runs/yeo3mt5w</a><br/> View project at: <a href='https://wandb.ai/sonder-art/llama_3_1_1B_complete' target=\"_blank\">https://wandb.ai/sonder-art/llama_3_1_1B_complete</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241126_003457-yeo3mt5w/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "x-0Fz0-1MGJt",
        "outputId": "686b344f-d8b7-4499-9da6-c028416402f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Before evaluation:\n",
            "Allocated: 1964.83 MB\n",
            "Cached: 6648.00 MB\n",
            "\n",
            "Before test evaluation:\n",
            "Allocated: 1964.83 MB\n",
            "Cached: 6648.00 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating test set:   0%|          | 0/62 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Evaluating test set: 100%|██████████| 62/62 [01:52<00:00,  1.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After test evaluation:\n",
            "Allocated: 1963.94 MB\n",
            "Cached: 2644.00 MB\n",
            "\n",
            "After evaluation:\n",
            "Allocated: 1963.94 MB\n",
            "Cached: 2644.00 MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Post-finetuning evaluation\n",
        "post_eval_results = evaluate_with_memory_tracking(trainer, lm_datasets, split=\"test\")\n",
        "\n",
        "# Visualization\n",
        "plot_metrics(trainer.state.log_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLvyBsavMGJt",
        "outputId": "d39e1d97-6724-4219-cde8-e39e2f25cf31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After finetuning: ['Once upon a time, in a small village nestled in the rolling hills of the countryside, there lived a young girl named Sophia. Sophia was a curious and adventurous child, with a mop of curly brown hair and a smile that could light up the']\n"
          ]
        }
      ],
      "source": [
        "# Generate text after finetuning\n",
        "post_finetune_output = generate_text(trainer.model.eval(), tokenizer, \"Once upon a time\")\n",
        "print(\"After finetuning:\", post_finetune_output)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cb56a516836042029ca0e30e71a8bcd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3f3ea508b964fcc8fd6a06f490dc584",
              "IPY_MODEL_9864f2559da0451c88fcc5053cd3660d"
            ],
            "layout": "IPY_MODEL_6e43e9eeb6d04567beb777dbae128554"
          }
        },
        "f3f3ea508b964fcc8fd6a06f490dc584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf05edfd78664dc8836e2e1dcf665163",
            "placeholder": "​",
            "style": "IPY_MODEL_c526f3e156e14a4783e32f610f14600f",
            "value": "0.023 MB of 0.023 MB uploaded\r"
          }
        },
        "9864f2559da0451c88fcc5053cd3660d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41b8256f02c24fd9a368afdd75995eb2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14b37e1b469544bcaa42d3470f0d2379",
            "value": 1
          }
        },
        "6e43e9eeb6d04567beb777dbae128554": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf05edfd78664dc8836e2e1dcf665163": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c526f3e156e14a4783e32f610f14600f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41b8256f02c24fd9a368afdd75995eb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14b37e1b469544bcaa42d3470f0d2379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}